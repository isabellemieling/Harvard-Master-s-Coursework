{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/AC 209A/STAT 121A Data Science: Homework 4\n",
    "**Harvard University**<br>\n",
    "**Fall 2016**<br>\n",
    "**Instructors: W. Pan, P. Protopapas, K. Rader**<br>\n",
    "**Due Date: ** Wednesday, October 5th, 2016 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the `IPython` notebook as well as the data file from Vocareum and complete locally.\n",
    "\n",
    "To submit your assignment, in Vocareum, upload (using the 'Upload' button on your Jupyter Dashboard) your solution to Vocareum as a single notebook with following file name format:\n",
    "\n",
    "`last_first_CourseNumber_HW4.ipynb`\n",
    "\n",
    "where `CourseNumber` is the course in which you're enrolled (CS 109a, Stats 121a, AC 209a). Submit your assignment in Vocareum using the 'Submit' button.\n",
    "\n",
    "**Avoid editing your file in Vocareum after uploading. If you need to make a change in a solution. Delete your old solution file from Vocareum and upload a new solution. Click submit only ONCE after verifying that you have uploaded the correct file. The assignment will CLOSE after you click the submit button.**\n",
    "\n",
    "Problems on homework assignments are equally weighted. The Challenge Question is required for AC 209A students and optional for all others. Student who complete the Challenge Problem as optional extra credit will receive +0.5% towards your final grade for each correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0: Basic Information\n",
    "\n",
    "Fill in your basic information. \n",
    "\n",
    "### Part (a): Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mieling, Isabelle\n",
    "\n",
    "### Part (b): Course Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AC 209a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Who did you work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[First and Land names of students with whom you have collaborated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data sets can be found in the ``datasets`` folder and are in comma separated value (CSV) format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Variable selection and regularization\n",
    "\n",
    "The data set for this problem is provided in ``dataset_1.txt`` and contains 10 predictors and a response variable.\n",
    "\n",
    "### Part (a): Analyze correlation among predictors\n",
    "- By visually inspecting the data set, do find that some of the predictors are correlated amongst themselves?\n",
    "\n",
    "\n",
    "- Compute the cofficient of correlation between each pair of predictors, and visualize the matrix of correlation coefficients using a heat map. Do the predictors fall naturally into groups based on the correlation values?\n",
    "\n",
    "\n",
    "- If you were asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, how many predictors will you pick, and which ones will you choose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.343727</td>\n",
       "      <td>0.524083</td>\n",
       "      <td>0.537768</td>\n",
       "      <td>0.435598</td>\n",
       "      <td>0.831999</td>\n",
       "      <td>0.153247</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>0.289394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.616969</td>\n",
       "      <td>0.616969</td>\n",
       "      <td>0.616969</td>\n",
       "      <td>0.287376</td>\n",
       "      <td>0.513844</td>\n",
       "      <td>0.497775</td>\n",
       "      <td>0.452732</td>\n",
       "      <td>0.914609</td>\n",
       "      <td>0.367390</td>\n",
       "      <td>0.444473</td>\n",
       "      <td>-0.277574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.995941</td>\n",
       "      <td>0.995941</td>\n",
       "      <td>0.995941</td>\n",
       "      <td>0.107294</td>\n",
       "      <td>0.097106</td>\n",
       "      <td>0.146751</td>\n",
       "      <td>0.136414</td>\n",
       "      <td>0.635926</td>\n",
       "      <td>0.535209</td>\n",
       "      <td>0.899457</td>\n",
       "      <td>-0.513097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.821732</td>\n",
       "      <td>0.821732</td>\n",
       "      <td>0.821732</td>\n",
       "      <td>0.202558</td>\n",
       "      <td>0.329504</td>\n",
       "      <td>0.359471</td>\n",
       "      <td>0.281453</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.479327</td>\n",
       "      <td>0.256271</td>\n",
       "      <td>-0.182353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.184564</td>\n",
       "      <td>0.270263</td>\n",
       "      <td>0.293385</td>\n",
       "      <td>0.263866</td>\n",
       "      <td>0.378630</td>\n",
       "      <td>0.740241</td>\n",
       "      <td>0.468589</td>\n",
       "      <td>-0.625117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.959357  0.959357  0.959357  0.343727  0.524083  0.537768  0.435598   \n",
       "1  0.616969  0.616969  0.616969  0.287376  0.513844  0.497775  0.452732   \n",
       "2  0.995941  0.995941  0.995941  0.107294  0.097106  0.146751  0.136414   \n",
       "3  0.821732  0.821732  0.821732  0.202558  0.329504  0.359471  0.281453   \n",
       "4  0.302423  0.302423  0.302423  0.184564  0.270263  0.293385  0.263866   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  0.831999  0.153247  0.005016  0.289394  \n",
       "1  0.914609  0.367390  0.444473 -0.277574  \n",
       "2  0.635926  0.535209  0.899457 -0.513097  \n",
       "3  0.106263  0.479327  0.256271 -0.182353  \n",
       "4  0.378630  0.740241  0.468589 -0.625117  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.loadtxt('dataset_1.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "# Split predictors and response\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAF6CAYAAAAJaaMjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJgy6JDXx/wEhGltPg0jMVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116b9b350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute matrix of correlation coefficients\n",
    "corr_matrix = np.corrcoef(x.T)\n",
    "\n",
    "# Display heat map \n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.pcolor(corr_matrix)\n",
    "\n",
    "ax.set_title('Heatmap of correlation matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analysis ** \n",
    "The heatmap above displays the coefficient of correlation between each pair of predictors. As can be seen in the heatmap above, there are areas of high positive and negative correlation among the coefficients with high, positive correlation in red along the center diagonal and high, negative correlation surrounding this. We see particularly high correlation on the region spanning the coefficients 0 to 7. Along the diagonal we see the coefficients of correlation between a coefficient and itself. Since the diagonal is a deep red, we see that there is high positive correlation of the coefficients amongst themselves. Predictors 0-2 have high positive correlation amongst themselves, high negative correlation with predictors 3-7 and low negative correlation with predictors 7-10. Predictors 3-7 have high positive correlation amongst themselves and low negative correlation with predicts 8-10. The predicts fall naturally in some groups of correlation. \n",
    "\n",
    "If asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, I would pick predictors 0, 5-8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Selecting minimal subset of predictors\n",
    "\n",
    "- Apply the variable selection methods discussed in class to choose a minimal subset of predictors that yield high prediction accuracy:\n",
    "    \n",
    "    - Exhaustive search\n",
    "    \n",
    "    - Step-wise forward selection **or** Step-wise backward selection  \n",
    "\n",
    "&emsp;&nbsp;&nbsp; In each method, use the Bayesian Information Criterion (BIC) to choose the subset size.\n",
    "\n",
    "- Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?\n",
    "\n",
    "**Note**: You may use the `statsmodels`'s `OLS` module to fit a linear regression model and evaluate BIC. You may **not** use library functions that implement variable selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best subset by exhaustive search:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### Best Subset Selection\n",
    "min_bic = 1e10 # set some initial large value for min BIC score\n",
    "best_subset = [] # best subset of predictors\n",
    "\n",
    "# Create all possible subsets of the set of 10 predictors\n",
    "predictor_set = set(range(10)) # predictor set = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "\n",
    "# Repeat for every possible size of subset\n",
    "for size_k in range(10): \n",
    "    # Create all possible subsets of size 'size', \n",
    "    # using the 'combination' function from the 'itertools' library\n",
    "    subsets_of_size_k = it.combinations(predictor_set, size_k + 1) # +1 becuase start at 0 in python \n",
    "    # output is list\n",
    "    \n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2 score\n",
    "    best_k_subset = [] # best subset of predictors of size k\n",
    "    \n",
    "    # Iterate over all subsets of our predictor set\n",
    "    for predictor_subset in subsets_of_size_k:     \n",
    "        # iterate through subsets\n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, predictor_subset]\n",
    "\n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Update max R^2 and best predictor subset of size k\n",
    "        # If current predictor subset has a higher R^2 score than that of the best subset \n",
    "        # we've found so far, remember the current predictor subset as the best!\n",
    "        if(r_squared > max_r_squared): \n",
    "            max_r_squared = r_squared\n",
    "            best_k_subset = predictor_subset[:]\n",
    "                \n",
    "\n",
    "    # Use only the best subset of size k for the predictors\n",
    "    x_subset = x[:, best_k_subset]\n",
    "        \n",
    "    # Fit and evaluate BIC of the best subset of size k\n",
    "    model = OLS(y, x_subset)\n",
    "    results = model.fit()\n",
    "    bic = results.bic\n",
    "    \n",
    "    # Update minimum BIC and best predictor subset\n",
    "    # If current predictor has a lower BIC score than that of the best subset \n",
    "    # we've found so far, remember the current predictor as the best!\n",
    "    if(bic < min_bic): \n",
    "        min_bic = bic\n",
    "        best_subset = best_k_subset[:]\n",
    "    \n",
    "print('Best subset by exhaustive search:')\n",
    "print sorted(best_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise forward subset selection:\n",
      "[0, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "### Step-wise Forward Selection\n",
    "d = x.shape[1] # total no. of predictors\n",
    "# x are the predictors so shape x gives number of predictors \n",
    "\n",
    "# Keep track of current set of chosen predictors, and the remaining set of predictors\n",
    "current_predictors = [] \n",
    "remaining_predictors = range(d)\n",
    "\n",
    "# Set some initial large value for min BIC score for all possible subsets\n",
    "global_min_bic = 1e10 \n",
    "\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, 0 predictors to d predictors\n",
    "for size in range(d):    \n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "    best_predictor = -1 # set some throwaway initial number for the best predictor to add\n",
    "    bic_with_best_predictor = 1e10 # set some initial large value for BIC score   \n",
    "        \n",
    "    # Iterate over all remaining predictors to find best predictor to add\n",
    "    for i in remaining_predictors:\n",
    "        # Make copy of current set of predictors\n",
    "        temp = current_predictors[:]\n",
    "        # Add predictor 'i'\n",
    "        temp.append(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, temp]\n",
    "        \n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if(r_squared > max_r_squared):\n",
    "            max_r_squared = r_squared\n",
    "            best_predictor = i\n",
    "            bic_with_best_predictor = results.bic\n",
    "    \n",
    "    # Remove best predictor from remaining list, and add best predictor to current list\n",
    "    remaining_predictors.remove(best_predictor)\n",
    "    current_predictors.append(best_predictor)\n",
    "    \n",
    "    # Check if BIC for with the predictor we just added is lower than \n",
    "    # the global minimum across all subset of predictors\n",
    "    if(bic_with_best_predictor < global_min_bic):\n",
    "        best_subset = current_predictors[:]\n",
    "        global_min_bic = bic_with_best_predictor\n",
    "    \n",
    "print 'Step-wise forward subset selection:'\n",
    "print sorted(best_subset) # add 1 as indices start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-wise backward subset selection:\n",
      "[2, 5, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "###  Step-wise Backward Selection\n",
    "d = x.shape[1] # total no. of predictors\n",
    "\n",
    "# Keep track of current set of chosen predictors\n",
    "current_predictors = range(d)\n",
    "\n",
    "# First, fit and evaluate BIC using all 'd' number of predictors\n",
    "model = OLS(y, x)\n",
    "results = model.fit()\n",
    "bic_all = results.bic\n",
    "\n",
    "# Set the minimum BIC score, initially, to the BIC score using all 'd' predictors\n",
    "global_min_bic = bic_all\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, d predictors to 1 predictor\n",
    "for size in range(d - 1, 1, -1): # stop before 0 to avoid choosing an empty set of predictors\n",
    "    max_r_squared = -1e10 # set some initial small value for max R^2\n",
    "    worst_predictor = -1 # set some throwaway initial number for the worst predictor to remove\n",
    "    bic_without_worst_predictor = 1e10 # set some initial large value for min BIC score  \n",
    "        \n",
    "    # Iterate over current set of predictors (for potential elimination)\n",
    "    for i in current_predictors:\n",
    "        # Create copy of current predictors, and remove predictor 'i'\n",
    "        temp = current_predictors[:]\n",
    "        temp.remove(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = x[:, temp]\n",
    "        \n",
    "        # Fit and evaluate R^2\n",
    "        model = OLS(y, x_subset)\n",
    "        results = model.fit()\n",
    "        r_squared = results.rsquared\n",
    "        \n",
    "        # Check if we get a higher R^2 value than than current max R^2, if so, update\n",
    "        if(r_squared > max_r_squared):\n",
    "            max_r_squared = r_squared\n",
    "            worst_predictor = i\n",
    "            bic_without_worst_predictor = results.bic\n",
    "          \n",
    "    # Remove worst predictor from current set of predictors\n",
    "    current_predictors.remove(worst_predictor)\n",
    "    \n",
    "    # Check if BIC for the predictor we just removed is lower than \n",
    "    # the global minimum across all subset of predictors\n",
    "    if(bic_without_worst_predictor < global_min_bic):\n",
    "        best_subset = current_predictors[:]\n",
    "        global_min_bic = bic_without_worst_predictor\n",
    "    \n",
    "print 'Step-wise backward subset selection:'\n",
    "print sorted(best_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analysis ** \n",
    "The chosen subset from the exhaustive search does not fully match that one chosen based on the correlation matrix from Part a. Predictors 0, 5, 7, and 8 are in both models but there is also some variability between the two chosen subsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Lasso and Ridge regression\n",
    "\n",
    "- Apply Lasso regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Apply Ridge regression with regularization parameter $\\lambda = 0.01$ and fit a regression model.\n",
    "\n",
    "    - Is there a difference between the model parameters you obtain different and those obtained from Lasso regression? If so, explain why.\n",
    "\n",
    "    - Identify the predictors that are assigned non-zero coefficients. Do these correspond to  the correlation matrix in Part (a)?\n",
    "\n",
    "\n",
    "- Is there anything peculiar that you observe about the coefficients Ridge regression assigns to the first three predictors? Do you observe the same with Lasso regression? Give an explanation for your observation.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to perform Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:\n",
      "Coefficients: [ 0.02717417  0.          0.         -0.         -0.02532806 -0.         -0.\n",
      "  0.04397321 -0.40612185 -0.22260474]\n",
      "Predictors with non-zero coefficients: [0, 4, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression\n",
    "reg = Lasso_Reg(alpha = 0.01)\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Lasso:'\n",
    "print 'Coefficients:', coefficients\n",
    "print  'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the predictors with non-zero coefficients and they do somewhat match the correlation matrix from Part a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge:\n",
      "Coefficients: [ 0.04353543  0.04353543  0.04353543  0.55217415 -0.19706852 -0.61421737\n",
      "  0.30484213  0.18742866 -0.50083242 -0.35908145]\n",
      "Predictors with non-zero coefficients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Ridge regression: Fit and evaluate \n",
    "reg = Ridge_Reg(alpha = 0.01)\n",
    "x[:,1] = x[:,0]\n",
    "x[:,2] = x[:,0]\n",
    "reg.fit(x, y)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "print 'Ridge:'\n",
    "print 'Coefficients:', coefficients\n",
    "print 'Predictors with non-zero coefficients:', [i for i, item in enumerate(coefficients) if abs(item) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Cross-validation and Bootstrapping\n",
    "In this problem, you will work with an expanded version of the automobile pricing data set you analyzed in Homework 3. The data set is contained ``dataset_2.txt``, with 26 attribues (i.e. predictors) for each automobile and corresponding prices. \n",
    "\n",
    "### Part(a): Encode categorical attributes and fill missing values\n",
    "Identify the categorical attributes in the data. Replace their values with the one-hot binary encoding. You may do this using the `get_dummies()` function in `pandas`. If you do this task correctly, you should get a total of 69 predictors after the encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Price is the last column --> is the outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of      horsepower  highway-mpg  symboling  normalized-losses           make  \\\n",
       "0          95.0         24.0   0.000000         120.232558         peugot   \n",
       "1         116.0         30.0   2.000000         134.000000         toyota   \n",
       "2         121.0         28.0   0.000000         188.000000            bmw   \n",
       "3         184.0         16.0   0.000000         120.232558  mercedes-benz   \n",
       "4         111.0         29.0   0.000000         102.000000         subaru   \n",
       "5          70.0         43.0   0.000000          81.000000      chevrolet   \n",
       "6          97.0         24.0   0.000000         161.000000         peugot   \n",
       "7         140.0         20.0   1.000000         158.000000           audi   \n",
       "8          86.0         33.0   0.000000          85.000000          honda   \n",
       "9          69.0         37.0   1.000000         128.000000         nissan   \n",
       "10        145.0         24.0   3.000000         120.232558     mitsubishi   \n",
       "11         70.0         34.0   1.000000         168.000000         toyota   \n",
       "12        123.0         25.0   0.000000          93.000000  mercedes-benz   \n",
       "13        112.0         29.0   1.000000         168.000000         toyota   \n",
       "14         68.0         38.0   1.000000         148.000000          dodge   \n",
       "15         97.0         34.0   0.000000         106.000000         nissan   \n",
       "16        110.0         28.0   2.000000         104.000000           saab   \n",
       "17        102.0         30.0   1.000000         118.000000          dodge   \n",
       "18        116.0         30.0   1.096045         137.000000     mitsubishi   \n",
       "19         68.0         38.0   0.000000         120.232558     volkswagen   \n",
       "20         69.0         37.0   1.000000         103.000000         nissan   \n",
       "21        145.0         24.0   3.000000         120.232558     mitsubishi   \n",
       "22         68.0         38.0   1.000000         148.000000          dodge   \n",
       "23        161.0         24.0   3.000000         197.000000         toyota   \n",
       "24         69.0         36.0   2.000000          83.000000         subaru   \n",
       "25        114.0         28.0   1.096045          95.000000          volvo   \n",
       "26         95.0         33.0   0.000000         161.000000         peugot   \n",
       "27         76.0         34.0   1.000000         101.000000          honda   \n",
       "28         69.0         37.0   2.000000         168.000000         nissan   \n",
       "29        116.0         30.0   2.000000         134.000000         toyota   \n",
       "..          ...          ...        ...                ...            ...   \n",
       "175       134.0         23.0   1.096045          95.000000          volvo   \n",
       "176       116.0         30.0   3.000000         153.000000     mitsubishi   \n",
       "177        92.0         32.0   1.096045          65.000000         toyota   \n",
       "178        97.0         24.0   0.000000         161.000000         peugot   \n",
       "179        94.0         32.0   0.000000         102.000000         subaru   \n",
       "180       102.0         30.0   1.000000         119.000000       plymouth   \n",
       "181        62.0         38.0   1.000000          87.000000         toyota   \n",
       "182        95.0         33.0   0.000000         161.000000         peugot   \n",
       "183        68.0         38.0   1.000000         154.000000       plymouth   \n",
       "184        70.0         34.0   0.000000          91.000000         toyota   \n",
       "185        70.0         37.0   0.000000          91.000000         toyota   \n",
       "186        85.0         34.0   2.000000          94.000000     volkswagen   \n",
       "187       161.0         24.0   3.000000         197.000000         toyota   \n",
       "188        68.0         38.0   2.000000         161.000000     mitsubishi   \n",
       "189       110.0         28.0   2.000000         104.000000           saab   \n",
       "190        70.0         34.0   0.000000          91.000000         toyota   \n",
       "191        95.0         24.0   0.000000         120.232558         peugot   \n",
       "192        69.0         37.0   2.000000         168.000000         nissan   \n",
       "193       111.0         23.0   0.000000          85.000000         subaru   \n",
       "194       207.0         25.0   3.000000         120.232558        porsche   \n",
       "195       121.0         28.0   0.000000         188.000000            bmw   \n",
       "196        92.0         34.0   1.096045          65.000000         toyota   \n",
       "197        68.0         38.0   1.000000         148.000000          dodge   \n",
       "198       162.0         22.0   1.096045          74.000000          volvo   \n",
       "199       114.0         28.0  -2.000000         103.000000          volvo   \n",
       "200       184.0         16.0   0.000000         120.232558  mercedes-benz   \n",
       "201        62.0         32.0   0.000000          91.000000         toyota   \n",
       "202        68.0         38.0   1.000000         154.000000       plymouth   \n",
       "203        82.0         25.0   0.000000         102.000000         subaru   \n",
       "204       116.0         30.0   3.000000         153.000000     mitsubishi   \n",
       "\n",
       "    fuel-type aspiration num-of-doors body-style drive-wheels   ...     \\\n",
       "0         gas        std         four      wagon          rwd   ...      \n",
       "1         gas        std          two    hardtop          rwd   ...      \n",
       "2         gas        std          two      sedan          rwd   ...      \n",
       "3         gas        std         four      sedan          rwd   ...      \n",
       "4         gas      turbo         four      sedan          4wd   ...      \n",
       "5         gas        std         four      sedan          fwd   ...      \n",
       "6         gas        std         four      sedan          rwd   ...      \n",
       "7         gas      turbo         four      sedan          fwd   ...      \n",
       "8         gas        std         four      sedan          fwd   ...      \n",
       "9         gas        std          two      sedan          fwd   ...      \n",
       "10        gas      turbo          two  hatchback          fwd   ...      \n",
       "11        gas        std          two  hatchback          rwd   ...      \n",
       "12     diesel      turbo          two    hardtop          rwd   ...      \n",
       "13        gas        std          two      sedan          rwd   ...      \n",
       "14        gas        std         four      sedan          fwd   ...      \n",
       "15        gas        std         four      sedan          fwd   ...      \n",
       "16        gas        std         four      sedan          fwd   ...      \n",
       "17        gas      turbo          two  hatchback          fwd   ...      \n",
       "18        gas        std         four      sedan          fwd   ...      \n",
       "19     diesel      turbo         four      sedan          fwd   ...      \n",
       "20        gas        std         four      wagon          fwd   ...      \n",
       "21        gas      turbo          two  hatchback          fwd   ...      \n",
       "22        gas        std         four  hatchback          fwd   ...      \n",
       "23        gas        std          two  hatchback          rwd   ...      \n",
       "24        gas        std          two  hatchback          fwd   ...      \n",
       "25        gas        std         four      sedan          rwd   ...      \n",
       "26     diesel      turbo         four      sedan          rwd   ...      \n",
       "27        gas        std          two  hatchback          fwd   ...      \n",
       "28        gas        std          two    hardtop          fwd   ...      \n",
       "29        gas        std          two    hardtop          rwd   ...      \n",
       "..        ...        ...          ...        ...          ...   ...      \n",
       "175       gas        std         four      sedan          rwd   ...      \n",
       "176       gas      turbo          two  hatchback          fwd   ...      \n",
       "177       gas        std         four  hatchback          fwd   ...      \n",
       "178       gas        std         four      sedan          rwd   ...      \n",
       "179       gas        std         four      sedan          fwd   ...      \n",
       "180       gas      turbo          two  hatchback          fwd   ...      \n",
       "181       gas        std          two  hatchback          fwd   ...      \n",
       "182    diesel      turbo         four      sedan          rwd   ...      \n",
       "183       gas        std         four      sedan          fwd   ...      \n",
       "184       gas        std         four  hatchback          fwd   ...      \n",
       "185       gas        std         four      sedan          fwd   ...      \n",
       "186       gas        std         four      sedan          fwd   ...      \n",
       "187       gas        std          two  hatchback          rwd   ...      \n",
       "188       gas        std          two  hatchback          fwd   ...      \n",
       "189       gas        std         four      sedan          fwd   ...      \n",
       "190       gas        std         four      sedan          fwd   ...      \n",
       "191       gas        std         four      wagon          rwd   ...      \n",
       "192       gas        std          two    hardtop          fwd   ...      \n",
       "193       gas      turbo         four      wagon          4wd   ...      \n",
       "194       gas        std          two    hardtop          rwd   ...      \n",
       "195       gas        std          two      sedan          rwd   ...      \n",
       "196       gas        std         four      sedan          fwd   ...      \n",
       "197       gas        std         four  hatchback          fwd   ...      \n",
       "198       gas      turbo         four      wagon          rwd   ...      \n",
       "199       gas        std         four      sedan          rwd   ...      \n",
       "200       gas        std         four      sedan          rwd   ...      \n",
       "201       gas        std         four      wagon          4wd   ...      \n",
       "202       gas        std         four      sedan          fwd   ...      \n",
       "203       gas        std         four      sedan          4wd   ...      \n",
       "204       gas      turbo          two  hatchback          fwd   ...      \n",
       "\n",
       "    engine-type  num-of-cylinders  engine-size  fuel-system  bore  stroke  \\\n",
       "0             l              four        120.0         mpfi  3.46    2.19   \n",
       "1           ohc              four        146.0         mpfi  3.62    3.50   \n",
       "2           ohc               six        164.0         mpfi  3.31    3.19   \n",
       "3          ohcv             eight        308.0         mpfi  3.80    3.35   \n",
       "4          ohcf              four        108.0         mpfi  3.62    2.64   \n",
       "5           ohc              four         90.0         2bbl  3.03    3.11   \n",
       "6             l              four        120.0         mpfi  3.46    3.19   \n",
       "7           ohc              five        131.0         mpfi  3.13    3.40   \n",
       "8           ohc              four        110.0         1bbl  3.15    3.58   \n",
       "9           ohc              four         97.0         2bbl  3.15    3.29   \n",
       "10          ohc              four        156.0         spdi  3.58    3.86   \n",
       "11          ohc              four         98.0         2bbl  3.19    3.03   \n",
       "12          ohc              five        183.0          idi  3.58    3.64   \n",
       "13         dohc              four         98.0         mpfi  3.24    3.08   \n",
       "14          ohc              four         90.0         2bbl  2.97    3.23   \n",
       "15          ohc              four        120.0         2bbl  3.33    3.47   \n",
       "16          ohc              four        121.0         mpfi  3.54    3.07   \n",
       "17          ohc              four         98.0         mpfi  3.03    3.39   \n",
       "18          ohc              four        110.0         spdi  3.17    3.46   \n",
       "19          ohc              four         97.0          idi  3.01    3.40   \n",
       "20          ohc              four         97.0         2bbl  3.15    3.29   \n",
       "21          ohc              four        156.0         spdi  3.59    3.86   \n",
       "22          ohc              four         90.0         2bbl  2.97    3.23   \n",
       "23         dohc               six        171.0         mpfi  3.27    3.35   \n",
       "24         ohcf              four         97.0         2bbl  3.62    2.36   \n",
       "25          ohc              four        141.0         mpfi  3.78    3.15   \n",
       "26            l              four        152.0          idi  3.70    3.52   \n",
       "27          ohc              four         92.0         1bbl  2.91    3.41   \n",
       "28          ohc              four         97.0         2bbl  3.15    3.29   \n",
       "29          ohc              four        146.0         mpfi  3.62    3.50   \n",
       "..          ...               ...          ...          ...   ...     ...   \n",
       "175        ohcv               six        173.0         mpfi  3.58    2.87   \n",
       "176         ohc              four        110.0         spdi  3.17    3.46   \n",
       "177         ohc              four        122.0         mpfi  3.31    3.54   \n",
       "178           l              four        120.0         mpfi  3.46    3.19   \n",
       "179        ohcf              four        108.0         mpfi  3.62    2.64   \n",
       "180         ohc              four         98.0         spdi  3.03    3.39   \n",
       "181         ohc              four         92.0         2bbl  3.05    3.03   \n",
       "182           l              four        152.0          idi  3.70    3.52   \n",
       "183         ohc              four         98.0         2bbl  2.97    3.23   \n",
       "184         ohc              four         98.0         2bbl  3.19    3.03   \n",
       "185         ohc              four         98.0         2bbl  3.19    3.03   \n",
       "186         ohc              four        109.0         mpfi  3.19    3.40   \n",
       "187        dohc               six        171.0         mpfi  3.27    3.35   \n",
       "188         ohc              four         92.0         2bbl  2.97    3.23   \n",
       "189         ohc              four        121.0         mpfi  3.54    3.07   \n",
       "190         ohc              four         98.0         2bbl  3.19    3.03   \n",
       "191           l              four        120.0         mpfi  3.46    2.19   \n",
       "192         ohc              four         97.0         2bbl  3.15    3.29   \n",
       "193        ohcf              four        108.0         mpfi  3.62    2.64   \n",
       "194        ohcf               six        194.0         mpfi  3.74    2.90   \n",
       "195         ohc               six        164.0         mpfi  3.31    3.19   \n",
       "196         ohc              four        122.0         mpfi  3.31    3.54   \n",
       "197         ohc              four         90.0         2bbl  2.97    3.23   \n",
       "198         ohc              four        130.0         mpfi  3.62    3.15   \n",
       "199         ohc              four        141.0         mpfi  3.78    3.15   \n",
       "200        ohcv             eight        308.0         mpfi  3.80    3.35   \n",
       "201         ohc              four         92.0         2bbl  3.05    3.03   \n",
       "202         ohc              four         90.0         2bbl  2.97    3.23   \n",
       "203        ohcf              four        108.0         2bbl  3.62    2.64   \n",
       "204         ohc              four        110.0         spdi  3.17    3.46   \n",
       "\n",
       "    compression-ratio peak-rpm  city-mpg    price  \n",
       "0                 8.4   5000.0      19.0  16695.0  \n",
       "1                 9.3   4800.0      24.0  11199.0  \n",
       "2                 9.0   4250.0      21.0  20970.0  \n",
       "3                 8.0   4500.0      14.0  40960.0  \n",
       "4                 7.7   4800.0      24.0  11259.0  \n",
       "5                 9.6   5400.0      38.0   6575.0  \n",
       "6                 8.4   5000.0      19.0  11900.0  \n",
       "7                 8.3   5500.0      17.0  23875.0  \n",
       "8                 9.0   5800.0      27.0   8845.0  \n",
       "9                 9.4   5200.0      31.0   5499.0  \n",
       "10                7.0   5000.0      19.0  12629.0  \n",
       "11                9.0   4800.0      29.0   8238.0  \n",
       "12               21.5   4350.0      22.0  28176.0  \n",
       "13                9.4   6600.0      26.0   9298.0  \n",
       "14                9.4   5500.0      31.0   6692.0  \n",
       "15                8.5   5200.0      27.0   9549.0  \n",
       "16                9.3   5250.0      21.0  15510.0  \n",
       "17                7.6   5500.0      24.0   7957.0  \n",
       "18                7.5   5500.0      23.0   9279.0  \n",
       "19               23.0   4500.0      33.0  13845.0  \n",
       "20                9.4   5200.0      31.0   7999.0  \n",
       "21                7.0   5000.0      19.0  14869.0  \n",
       "22                9.4   5500.0      31.0   6229.0  \n",
       "23                9.3   5200.0      19.0  15998.0  \n",
       "24                9.0   4900.0      31.0   5118.0  \n",
       "25                9.5   5400.0      23.0  16845.0  \n",
       "26               21.0   4150.0      28.0  13200.0  \n",
       "27                9.2   6000.0      30.0   7129.0  \n",
       "28                9.4   5200.0      31.0   8249.0  \n",
       "29                9.3   4800.0      24.0   9639.0  \n",
       "..                ...      ...       ...      ...  \n",
       "175               8.8   5500.0      18.0  21485.0  \n",
       "176               7.5   5500.0      23.0   9959.0  \n",
       "177               8.7   4200.0      27.0  11248.0  \n",
       "178               8.4   5000.0      19.0  11900.0  \n",
       "179               9.0   5200.0      26.0   9960.0  \n",
       "180               7.6   5500.0      24.0   7957.0  \n",
       "181               9.0   4800.0      31.0   6338.0  \n",
       "182              21.0   4150.0      28.0  17950.0  \n",
       "183               9.4   5500.0      31.0   7609.0  \n",
       "184               9.0   4800.0      28.0   8358.0  \n",
       "185               9.0   4800.0      30.0   6938.0  \n",
       "186               9.0   5250.0      27.0   8195.0  \n",
       "187               9.3   5200.0      20.0  16558.0  \n",
       "188               9.4   5500.0      31.0   6189.0  \n",
       "189               9.3   5250.0      21.0  15510.0  \n",
       "190               9.0   4800.0      28.0   9258.0  \n",
       "191               8.4   5000.0      19.0  16695.0  \n",
       "192               9.4   5200.0      31.0   8249.0  \n",
       "193               7.7   4800.0      23.0  11694.0  \n",
       "194               9.5   5900.0      17.0  32528.0  \n",
       "195               9.0   4250.0      21.0  20970.0  \n",
       "196               8.7   4200.0      29.0   8948.0  \n",
       "197               9.4   5500.0      31.0   6229.0  \n",
       "198               7.5   5100.0      17.0  18950.0  \n",
       "199               9.5   5400.0      23.0  12940.0  \n",
       "200               8.0   4500.0      14.0  40960.0  \n",
       "201               9.0   4800.0      27.0   8778.0  \n",
       "202               9.4   5500.0      31.0   6692.0  \n",
       "203               9.0   4800.0      24.0   9233.0  \n",
       "204               7.5   5500.0      23.0   9959.0  \n",
       "\n",
       "[205 rows x 26 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_df = pd.read_csv('dataset_2.txt')\n",
    "x_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horsepower</th>\n",
       "      <th>highway-mpg</th>\n",
       "      <th>-2.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>1.09604519774</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>normalized-losses</th>\n",
       "      <th>audi</th>\n",
       "      <th>...</th>\n",
       "      <th>2bbl</th>\n",
       "      <th>idi</th>\n",
       "      <th>mpfi</th>\n",
       "      <th>spdi</th>\n",
       "      <th>bore</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compression-ratio</th>\n",
       "      <th>peak-rpm</th>\n",
       "      <th>city-mpg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>2.19</td>\n",
       "      <td>8.4</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.50</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.31</td>\n",
       "      <td>3.19</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4250.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.232558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.35</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>40960.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.64</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.11</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6575.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.19</td>\n",
       "      <td>8.4</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>140.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.3</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23875.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.58</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5800.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8845.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.29</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5499.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   horsepower  highway-mpg  -2.0  0.0  1.0  1.09604519774  2.0  3.0  \\\n",
       "0        95.0         24.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "1       116.0         30.0   0.0  0.0  0.0            0.0  1.0  0.0   \n",
       "2       121.0         28.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "3       184.0         16.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "4       111.0         29.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "5        70.0         43.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "6        97.0         24.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "7       140.0         20.0   0.0  0.0  1.0            0.0  0.0  0.0   \n",
       "8        86.0         33.0   0.0  1.0  0.0            0.0  0.0  0.0   \n",
       "9        69.0         37.0   0.0  0.0  1.0            0.0  0.0  0.0   \n",
       "\n",
       "   normalized-losses  audi   ...     2bbl  idi  mpfi  spdi  bore  stroke  \\\n",
       "0         120.232558   0.0   ...      0.0  0.0   1.0   0.0  3.46    2.19   \n",
       "1         134.000000   0.0   ...      0.0  0.0   1.0   0.0  3.62    3.50   \n",
       "2         188.000000   0.0   ...      0.0  0.0   1.0   0.0  3.31    3.19   \n",
       "3         120.232558   0.0   ...      0.0  0.0   1.0   0.0  3.80    3.35   \n",
       "4         102.000000   0.0   ...      0.0  0.0   1.0   0.0  3.62    2.64   \n",
       "5          81.000000   0.0   ...      1.0  0.0   0.0   0.0  3.03    3.11   \n",
       "6         161.000000   0.0   ...      0.0  0.0   1.0   0.0  3.46    3.19   \n",
       "7         158.000000   1.0   ...      0.0  0.0   1.0   0.0  3.13    3.40   \n",
       "8          85.000000   0.0   ...      0.0  0.0   0.0   0.0  3.15    3.58   \n",
       "9         128.000000   0.0   ...      1.0  0.0   0.0   0.0  3.15    3.29   \n",
       "\n",
       "   compression-ratio  peak-rpm  city-mpg    price  \n",
       "0                8.4    5000.0      19.0  16695.0  \n",
       "1                9.3    4800.0      24.0  11199.0  \n",
       "2                9.0    4250.0      21.0  20970.0  \n",
       "3                8.0    4500.0      14.0  40960.0  \n",
       "4                7.7    4800.0      24.0  11259.0  \n",
       "5                9.6    5400.0      38.0   6575.0  \n",
       "6                8.4    5000.0      19.0  11900.0  \n",
       "7                8.3    5500.0      17.0  23875.0  \n",
       "8                9.0    5800.0      27.0   8845.0  \n",
       "9                9.4    5200.0      31.0   5499.0  \n",
       "\n",
       "[10 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.shape(x_df)[1]\n",
    "# Record start index of attribute in expanded feature vector \n",
    "start_index = np.zeros(d+1) # last entry would contain the len of vector + 1 \n",
    "\n",
    "# Create a new data frame to store one-hot encoding of attributes\n",
    "\n",
    "x_df_expanded = pd.DataFrame({})\n",
    "\n",
    "# Iterate over all attributes\n",
    "for column in x_df.columns:\n",
    "    # check if attribute is categorical, has less than 8 unique values, or is string values\n",
    "    if len(x_df[column].unique()) < 8 or x_df[column].dtype == np.dtype('object'):\n",
    "        encoding = pd.get_dummies(x_df[column])\n",
    "        x_df_expanded = pd.concat([x_df_expanded, encoding], axis=1)\n",
    "    else:\n",
    "        x_df_expanded = pd.concat([x_df_expanded, x_df[[column]]], axis=1)\n",
    "\n",
    "x_df_expanded.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(x_df_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print len(x_df_expanded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Analysis ** We have 69 predictors, leaving a total of 70 variables in the dataset. We currently have a Pandas DataFrame with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Apply regular linear regression\n",
    "- Split the data set into train and test sets, with the first 25% of the data for training and the remaining for testing.  \n",
    "\n",
    "\n",
    "- Use regular linear regression to fit a model to the training set and evaluate the R^2 score of the fitted model on both the training and test sets. What do you observe about these values?\n",
    "\n",
    "\n",
    "- You had seen in class that the R^2 value of a least-squares fit to a data set would lie between 0 and 1. Is this true for the test R^2 values reported above? If not, give a reason for why this is the case.\n",
    "\n",
    "\n",
    "- Is there a need for regularization while fitting a linear model to this data set?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numpy = x_df_expanded.as_matrix(columns=None)\n",
    "twent_five = int(len(df_numpy)*(0.25))\n",
    "training, test = df_numpy[:twent_five,:], df_numpy[twent_five:,:]\n",
    "\n",
    "x_train = training[:,:-1]\n",
    "y_train = training[:,-1]\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Value for Training Set:  1.0\n",
      "R^2 Value for Testing Set:  -5.97542556681\n"
     ]
    }
   ],
   "source": [
    "linmodel = Lin_Reg()\n",
    "linmodel = linmodel.fit(x_train, y_train)\n",
    "predicted_y = linmodel.predict(x_test)\n",
    "\n",
    "r_train_plain = linmodel.score(x_train, y_train)\n",
    "r_test_plain = linmodel.score(x_test, y_test)\n",
    "\n",
    "print 'R^2 Value for Training Set: ' , r_train_plain\n",
    "print 'R^2 Value for Testing Set: ' , r_test_plain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe that the R^2 value for the training set is 1 and that for the testing set is about -6. No, here the R^2 value for the testing set is a negative number. Here, the R^2 value might be a negative number because the chosen model does not follow the trend of the data and actually fits worse than a horizontal line. There may be a need for regularization while fitting the linear model to this data set.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Apply Ridge regression\n",
    "\n",
    "- Apply Ridge regression on the training set for different values of the regularization parameter $\\lambda$ in the range $\\{10^{-7}, 10^{-6}, \\ldots, 10^7\\}$. Evaluate the R^2 score for the models you obtain on both the train and test sets. Plot both values as a function of $\\lambda$. \n",
    "\n",
    "\n",
    "- Explain the relationship between the regularization parameter and the training and test R^2 scores.\n",
    "\n",
    "\n",
    "- How does the best test R^2 value obtained using Ridge regression compare with that of plain linear regression? Explain.\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a ridge regression model and evaluate the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "51 51 154 154\n",
      "(51, 69)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "print type(x_train)\n",
    "print len(x_train) ,len( y_train), len(x_test), len( y_test)\n",
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge:\n",
      "Coefficients: [  3.78622144e+01  -1.47840026e+02   0.00000000e+00   1.11153760e+03\n",
      "   9.05521228e+02  -1.08386417e+03  -4.82893771e+02  -4.50300894e+02\n",
      "   1.02060279e+01   3.39461486e+02   1.14809954e+03   2.90297979e+02\n",
      "  -1.16043239e+03  -6.43051423e+02  -3.13196476e+02   1.11989129e+03\n",
      "   0.00000000e+00   3.26619761e+01  -7.48477118e+02  -9.01220371e+02\n",
      "  -3.29496183e+02   1.43613057e+03   0.00000000e+00   9.00750568e+02\n",
      "  -8.83863020e+02  -1.07935148e+03   9.64783961e+02  -1.72988907e+02\n",
      "  -1.36545082e+02   1.36545082e+02  -2.59827342e+02   2.59827342e+02\n",
      "   3.40545588e+02  -3.40545588e+02   0.00000000e+00   8.44060185e+02\n",
      "  -1.92188672e+02  -5.03324943e+02  -1.48546570e+02   1.57969879e+02\n",
      "   1.34141927e+02  -2.92111806e+02  -1.43613057e+03   1.43613057e+03\n",
      "   1.08927263e+02  -1.84792525e+01   1.12704541e+02   2.64132743e+02\n",
      "  -1.82706484e-01  -1.96239611e+03  -9.01220371e+02   1.62494084e+03\n",
      "   5.52267551e+02   6.86408091e+02   6.86408091e+02   7.72944683e+02\n",
      "  -1.75402198e+03   2.94669210e+02   9.54106133e+01  -6.43051423e+02\n",
      "   8.63855401e+02  -1.36545082e+02   4.58733033e+01  -1.30132199e+02\n",
      "  -9.13643922e+02  -3.14990054e+03   4.19826103e+02   2.40470206e+00\n",
      "  -1.63915023e+02]\n",
      "Selected predictors: [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]\n",
      "Ridge Regression: R^2 score on training set 0.984459334945\n",
      "Ridge Regression: R^2 score on test set 0.913535120275\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression from lecture 7 \n",
    "reg = Ridge_Reg(alpha = 1.0)\n",
    "reg.fit(x_train, y_train)\n",
    "coefficients = reg.coef_\n",
    "\n",
    "predictors = [i for i, item in enumerate(coefficients) if abs(item) > 0]\n",
    "\n",
    "print 'Ridge:'\n",
    "print 'Coefficients:', coefficients\n",
    "print  'Selected predictors:',predictors  \n",
    "print 'Ridge Regression: R^2 score on training set', reg.score(x_train, y_train)\n",
    "print 'Ridge Regression: R^2 score on test set', reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run for different values of lambda \n",
    "lambda_min = -7\n",
    "lambda_max = 7\n",
    "\n",
    "num_lambdas = 1000\n",
    "num_predictors = x_train.shape[1]\n",
    "\n",
    "lambdas= np.linspace(lambda_min,lambda_max, num_lambdas)\n",
    "\n",
    "train_r_squared = np.zeros(num_lambdas)\n",
    "test_r_squared = np.zeros(num_lambdas)\n",
    "\n",
    "coeff_a =np.zeros((num_lambdas, num_predictors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ind, i in enumerate(lambdas):    \n",
    "    # Fit ridge regression on train set\n",
    "    reg = Ridge_Reg(alpha = 10**i)\n",
    "    reg.fit(x_train, y_train)\n",
    "       \n",
    "    coeff_a[ind,:] = reg.coef_\n",
    "    # Evaluate train & test performance\n",
    "    train_r_squared[ind] = reg.score(x_train, y_train)\n",
    "    test_r_squared[ind] = reg.score(x_test, y_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_r_squared_plain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e409f706e603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$R^2$ score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_r_squared_plain\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_r_squared_plain' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAH3CAYAAAC8SfapAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e528390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train an test R-squared as a function parameter value\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "ax.axhline(y = r_test_plain, c='g', label = 'Plain Regression')\n",
    "ax.semilogx(10.0**lambdas, (train_r_squared), c='b', label='Ridge: Train')\n",
    "ax.semilogx(10.0**lambdas, (test_r_squared), c='r', label='Ridge: Test')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Regularization parameter $\\lambda$')\n",
    "ax.set_ylabel(r'$R^2$ score')\n",
    "\n",
    "ax.set_ylim((test_r_squared_plain-0.2, 1.2))\n",
    "ax.legend(loc = 'best')\n",
    "\n",
    "print 'Ridge Regression: max R^2 score on training set', max(train_r_squared)\n",
    "print 'Ridge Regression: max R^2 score on test set', max(test_r_squared)\n",
    "\n",
    "reg = Lasso_Reg(alpha =lambdas[np.argmax(test_r_squared)])\n",
    "reg.fit(x_train, y_train)\n",
    "coefficients = reg.coef_\n",
    "print coefficients\n",
    "print  'Selected predictors:', [i for i, item in enumerate(coefficients) if abs(item) > 0]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (d): Tune regularization parameter using cross-validation and bootstrapping\n",
    "-  Evaluate the performance of the Ridge regression for different regularization parameters $\\lambda$ using 5-fold cross validation **or** bootstrapping on the training set. \n",
    "\n",
    "    - Plot the cross-validation (CV) or bootstrapping R^2 score as a function of $\\lambda$. \n",
    "    \n",
    "    - How closely does the CV score or bootstrapping score match the R^2 score on the test set? Does the model with lowest CV score or bootstrapping score correspond to the one with maximum R^2 on the test set?\n",
    "    \n",
    "    - Does the model chosen by CV or bootstrapping perform better than plain linear regression?\n",
    "\n",
    "**Note**: You may use the `statsmodels` or `sklearn` to fit a linear regression model and evaluate the fits. You may also use `kFold` from `sklearn.cross_validation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotR():\n",
    "    # training set = training\n",
    "    # x_train, y_train\n",
    "    \n",
    "    # size of data set\n",
    "    n = x_train.shape[0]\n",
    "    \n",
    "    # number of subsamples \n",
    "    num_samples = 200\n",
    "    \n",
    "    # Ridge Regression \n",
    "    \n",
    "    # coefficients \n",
    "    # Create a n  array to store coefficients for all subsamples\n",
    "    coefs_a = np.zeros((num_samples, 69))\n",
    "    # array with all 0s\n",
    "    # 200x2, 200 samples with 2 columns\n",
    "    # bootstrapping 200 times \n",
    "    # have 69 predictors \n",
    "    \n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # Generate a random subsample of data points\n",
    "        # This is the bootstrap \n",
    "        # \n",
    "        perm =np.random.randint(low=0, high=n, size=50) # Generate a list of indices 0 to n and permute it\n",
    "        # size 50 \n",
    "        x_subsample = x_train[perm] \n",
    "        y_subsample = y_train[perm]\n",
    "        # from a random set of n integers, I take those indices fro mmy x \n",
    "        # perm is a listof random numbers, and i take those as indices to create the subsample\n",
    "        # this will pick out rows\n",
    "        \n",
    "        # Fit ridge regression for all parameters \n",
    "        reg = Ridge_Reg(alpha = 1.0)\n",
    "        reg.fit(x_subsample, y_subsample)\n",
    "        coefficients = reg.coef_\n",
    "        predictors = [i for i, item in enumerate(coefficients) if abs(item) > 0]\n",
    "        \n",
    "        lambda_min = -7\n",
    "        lambda_max = 7\n",
    "\n",
    "        num_lambdas = 1000\n",
    "        num_predictors = x.shape[1]\n",
    "\n",
    "        lambdas= np.linspace(lambda_min,lambda_max, num_lambdas)\n",
    "\n",
    "        train_r_squared = np.zeros(num_lambdas)\n",
    "        test_r_squared = np.zeros(num_lambdas)\n",
    "        \n",
    "        for ind, i in enumerate(lambdas):    \n",
    "        # Fit ridge regression on train set\n",
    "            reg = Ridge_Reg(alpha = 10**i)\n",
    "            reg.fit(x_train, y_train)\n",
    "       \n",
    "            coeff_a[ind,:] = reg.coef_\n",
    "            # Evaluate train & test performance\n",
    "            train_r_squared[ind] = reg.score(x_train, y_train)\n",
    "            test_r_squared[ind] = reg.score(x_test, y_test) \n",
    "\n",
    "# need to store parameters 200 times, 69 parameters\n",
    "# take average of each of htese parameters\n",
    "# keep track of the parameter that gives the best R^2 value\n",
    "\n",
    "        \n",
    "\n",
    "    # HOW DOES IT COMPARED TO STANDARD CONF INTERVAL WITHOUT BOOTSTRAP ?\n",
    "  \n",
    "    print \"Theoretical estimate: SE beta_0\", np.sqrt(model_results.mse_resid*(1.0/n+ x.mean()**2/np.sum( (x-x.mean())**2)))\n",
    "    print \"Theoretical estimate: SE beta_1\", np.sqrt(model_results.mse_resid/np.sum( (x-x.mean())**2))\n",
    "    \n",
    "    # Repeat for each coefficient (this is just the confidence intervals )\n",
    "    for j in range(69):\n",
    "        # Compute mean for the j-th coefficent from subsamples\n",
    "        coef_j_mean = np.mean(coefs_a[:, j])\n",
    "        print coef_j_mean\n",
    "        # Compute confidence interval at 95% confidence level (use formula!)\n",
    "        conf_int_left = np.percentile(coefs_a[:, j], 2.5)\n",
    "        conf_int_right = np.percentile(coefs_a[:, j], 97.5)\n",
    "        print('Bootstrap: SE'+ r' beta_'+str(j) +' =',(coef_j_mean-conf_int_left) /2) \n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Ridge regression *via* ordinary least-squares regression\n",
    "\n",
    "We present an approach to implement Ridge regression using oridinary least-squares regression. Given a matrix of responses $\\mathbf{X} \\in \\mathbb{R}^{n\\times p}$ and response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$, one can implement Ridge regression with regularization parameter $\\lambda$ as follows:\n",
    "\n",
    "- Augment the matrix of predictors $\\mathbf{X}$ with $p$ new rows containing the scaled identity matrix $\\sqrt{\\lambda}\\mathbf{I} \\in \\mathbb{R}^{p \\times p}$, i.e.\n",
    "$$\\overline{\\mathbf{X}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "X_{11} & \\ldots & X_{1p}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "X_{n1} & \\ldots & X_{np}\\\\\n",
    "\\sqrt{\\lambda} & \\ldots & 0\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "0 & \\ldots & \\sqrt{\\lambda}\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{(n+p)\\times p}\n",
    ".\n",
    "$$\n",
    "\n",
    "\n",
    "- Augment the response vector $\\mathbf{y}$ with a column of $p$ zeros, i.e.\n",
    "$$\n",
    "\\overline{\\mathbf{y}} \\,=\\, \n",
    "\\begin{bmatrix}\n",
    "y_{1}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\\,\\in\\,\n",
    "\\mathbb{R}^{n+p}.\n",
    "$$\n",
    "\n",
    "\n",
    "- Apply ordinary least-squares regression on the augmented data set $(\\overline{\\mathbf{X}}, \\overline{\\mathbf{y}})$.\n",
    "\n",
    "### Part (a): Show the proposed approach implements Ridge regression\n",
    "Show that the approach proposed above implements Ridge regression with parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# score is all wrong\n",
    "# conctenate zeros, same as for y test as x tesrt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Debug our implementation of ridge regression\n",
    "You're a grader for CS109A, the following is an implemention of Ridge regression (via the above approach) submitted by a student. The dataset is ``dataset_3.txt``. The regression model is fitted to a training set, and the R^2 scores of the fitted model on the training and test sets are plotted as a function of the regularization parameter. Grade this solution according to the following rubric (each category is equally weighted): \n",
    "\n",
    "- correctness\n",
    "\n",
    "- interpretation (if applicable)\n",
    "\n",
    "- code/algorithm design\n",
    "\n",
    "- presentation\n",
    "\n",
    "In addition to providing an holistic grade (between 0 to 5), provide a corrected version of this code that is submission quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code receives 3/5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1149841d0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114906290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit\n",
    "def ridge(x_train, y_train, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_train=np.concatenate((x_train,reg_param*np.identity(n)),axis=1)\n",
    "    y_train_=np.zeros((n+np.shape(x_train)[1],1))\n",
    "    for c in range(n):\n",
    "        y_train_[c]= y_train[c]\n",
    "    import sklearn\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "    model.fit(x_train,y_train.reshape(-1,1))\n",
    "    return model\n",
    "\n",
    "# Score\n",
    "def score(m,x_test,y_test, reg_param):\n",
    "    n=np.shape(x_train)[0]\n",
    "    x_test=np.concatenate((x_test,reg_param*np.identity(n)),axis=1)\n",
    "    y_test_=np.zeros((n+np.shape(x_test)[1],1))\n",
    "    for c in range(n):\n",
    "        y_test_[c]= y_test[c]\n",
    "    return m.score(x_test,y_test.reshape(-1,1))\n",
    "\n",
    "# Load\n",
    "data = np.loadtxt('datasets/dataset_3.txt', delimiter=',')\n",
    "n = data.shape[0]\n",
    "n = int(np.round(n*0.5))\n",
    "x_train = data[0:n,0:100]\n",
    "y_train = data[0:n,100]\n",
    "x_test = data[n:2*n,0:100]\n",
    "y_test = data[n:2*n,100]\n",
    "\n",
    "# Params\n",
    "a=np.zeros(5)\n",
    "for i in range(-2,2):\n",
    "    a[i+2]=10**i\n",
    "\n",
    "# Iterate\n",
    "rstr =np.zeros(5)\n",
    "rsts =np.zeros(5)\n",
    "for j in range(0,5):    \n",
    "    m =ridge(x_train,y_train,a[i])\n",
    "    rstr[j]=score(m,x_train,y_train,a[j])\n",
    "    rsts[i]=score(m,x_test,y_test,a[i])\n",
    "\n",
    "# Plot\n",
    "plt.plot(a,rstr)\n",
    "plt.plot(a,rsts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "def ridge(x_train, y_train, reg_param):\n",
    "    n=np.shape(x_train)[1]\n",
    "    x_train=np.concatenate((x_train,reg_param*np.identity(n)),axis=1)\n",
    "    y_train_=np.zeros((n+np.shape(x_train)[1],1))\n",
    "    for c in range(n):\n",
    "        y_train_[c]= y_train[c]\n",
    "    import sklearn\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "    model.fit(x_train,y_train.reshape(-1,1))\n",
    "    return model\n",
    "\n",
    "# Score\n",
    "def score(w, c, x_test, y_test):\n",
    "    # Compute predicted y values\n",
    "    y_predict = np.dot(x_test, w) + c\n",
    "    \n",
    "    # Evaluate square error RSS\n",
    "    sq_error = np.sum(np.square(y_predict - y_test))\n",
    "    \n",
    "    # Evaluate squared error for predicting mean  TSS\n",
    "    y_mean = np.mean(y_test)\n",
    "    y_variance = np.sum(np.square(y_mean - y_test))\n",
    "    \n",
    "    # Evaluate R^2 score values\n",
    "    r_squared = 1 - sq_error / y_variance\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Load\n",
    "data = np.loadtxt('dataset_3.txt', delimiter=',')\n",
    "n = data.shape[1]\n",
    "n = int(np.round(n*0.5))\n",
    "x_train = data[0:n,0:100]\n",
    "y_train = data[0:n,100]\n",
    "x_test = data[n:2*n,0:100]\n",
    "y_test = data[n:2*n,100]\n",
    "\n",
    "# Params\n",
    "a=np.zeros(5)\n",
    "for i in range(-2,2):\n",
    "    a[i+2]=10**i\n",
    "\n",
    "# Iterate\n",
    "rstr =np.zeros(5)\n",
    "rsts =np.zeros(5)\n",
    "for j in range(0,5):    \n",
    "    m =ridge(x_train,y_train,a[i])\n",
    "    rstr[j]=score(m,x_train,y_train,a[j])\n",
    "    rsts[i]=score(m,x_test,y_test,a[i])\n",
    "\n",
    "# Plot\n",
    "plt.plot(a,rstr)\n",
    "plt.plot(a,rsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Altered Code ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Predicting Outcome of a Fund-raising Campaign\n",
    "You are provided a data set containing details of mail sent to 95,412 potential donors for a fund-raising campaign of a not-for-profit organization. This data set also contains the amount donated by each donor. The task is to build a model that can estimate the amount that a donor would donate using his/her attributes. The data is contained in the file `dataset_4.txt`. Each row contains 376 attributes for a donor, followed by the donation amount.\n",
    "\n",
    "**Note**: For additional information about the attributes used, please look up the file `dataset_4_description.txt`. This files also contains details of attributes that have been omitted from the data set.\n",
    "\n",
    "### Part (a): Fit regression model\n",
    "Build a suitable model to predict the donation amount. How good is your model? \n",
    "\n",
    "\n",
    "### Part (b): Evaluate the total profit of the fitted model\n",
    "Suppose you are told that the cost of mailing the donor is \\$7. Use your model to maximize profit. Implement, explain and rigorously justify your strategy. How does your strategry compare with blanket mailing everyone.\n",
    "\n",
    "### Part (c): Further Discussion\n",
    "In hindsight, thoroughly discuss the appropriatenes of using a regression model for this dataset (you must at least address the suitability with respect to profit maximization and model assumptions). Rigorously justify your reasoning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBK</th>\n",
       "      <th>2.0</th>\n",
       "      <th>MN</th>\n",
       "      <th>_</th>\n",
       "      <th>0</th>\n",
       "      <th>_.1</th>\n",
       "      <th>_.2</th>\n",
       "      <th>_.3</th>\n",
       "      <th>_.4</th>\n",
       "      <th>XXXX</th>\n",
       "      <th>...</th>\n",
       "      <th>1.0.31</th>\n",
       "      <th>L</th>\n",
       "      <th>3.0.11</th>\n",
       "      <th>D</th>\n",
       "      <th>X.1</th>\n",
       "      <th>X.2</th>\n",
       "      <th>X.3</th>\n",
       "      <th>3.0.12</th>\n",
       "      <th>A</th>\n",
       "      <th>4.0.12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>14.0</td>\n",
       "      <td>A</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>11.0</td>\n",
       "      <td>C</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>22.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>38.0</td>\n",
       "      <td>B</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BBK  2.0  MN  _  0 _.1 _.2 _.3 _.4  XXXX  ...   1.0.31  L 3.0.11  D X.1  \\\n",
       "0  SYN  0.0  TX  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  D   X   \n",
       "1  DRK  0.0  IA  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  D   X   \n",
       "2  BHG  0.0  CA  _  0   _   _   _   _  XXXX  ...      0.0  L    2.0  F   X   \n",
       "3  L01  1.0  GA  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  E   X   \n",
       "4  ARG  0.0  VA  _  0   _   _   _   _  XXXX  ...      0.0  L    3.0  E   X   \n",
       "\n",
       "  X.2 X.3 3.0.12  A 4.0.12  \n",
       "0   X   X   14.0  A    7.0  \n",
       "1   X   X   11.0  C    5.0  \n",
       "2   X   X    2.0  A   13.0  \n",
       "3   X   X   22.0  A   10.0  \n",
       "4   X   X   38.0  B    5.0  \n",
       "\n",
       "[5 rows x 377 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data2  = pd.read_csv('dataset_4.txt', delimiter=',', skiprows=1)\n",
    "\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n"
     ]
    }
   ],
   "source": [
    "print len(data2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BBK</th>\n",
       "      <th>2.0</th>\n",
       "      <th>MN</th>\n",
       "      <th>_</th>\n",
       "      <th>0</th>\n",
       "      <th>_.1</th>\n",
       "      <th>_.2</th>\n",
       "      <th>_.3</th>\n",
       "      <th>_.4</th>\n",
       "      <th>XXXX</th>\n",
       "      <th>...</th>\n",
       "      <th>1.0.31</th>\n",
       "      <th>L</th>\n",
       "      <th>3.0.11</th>\n",
       "      <th>D</th>\n",
       "      <th>X.1</th>\n",
       "      <th>X.2</th>\n",
       "      <th>X.3</th>\n",
       "      <th>3.0.12</th>\n",
       "      <th>A</th>\n",
       "      <th>4.0.12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SYN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>14.0</td>\n",
       "      <td>A</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRK</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>11.0</td>\n",
       "      <td>C</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>22.0</td>\n",
       "      <td>A</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARG</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>XXXX</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>L</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>38.0</td>\n",
       "      <td>B</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BBK  2.0  MN  _  0 _.1 _.2 _.3 _.4  XXXX  ...   1.0.31  L 3.0.11  D X.1  \\\n",
       "0  SYN  0.0  TX  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  D   X   \n",
       "1  DRK  0.0  IA  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  D   X   \n",
       "2  BHG  0.0  CA  _  0   _   _   _   _  XXXX  ...      0.0  L    2.0  F   X   \n",
       "3  L01  1.0  GA  _  0   _   _   _   _  XXXX  ...      1.0  L    3.0  E   X   \n",
       "4  ARG  0.0  VA  _  0   _   _   _   _  XXXX  ...      0.0  L    3.0  E   X   \n",
       "\n",
       "  X.2 X.3 3.0.12  A 4.0.12  \n",
       "0   X   X   14.0  A    7.0  \n",
       "1   X   X   11.0  C    5.0  \n",
       "2   X   X    2.0  A   13.0  \n",
       "3   X   X   22.0  A   10.0  \n",
       "4   X   X   38.0  B    5.0  \n",
       "\n",
       "[5 rows x 377 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data2)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = np.shape(data)[1]\n",
    "# Record start index of attribute in expanded feature vector \n",
    "start_index = np.zeros(d+1) # last entry would contain the len of vector + 1 \n",
    "\n",
    "# Create a new data frame to store one-hot encoding of attributes\n",
    "\n",
    "x_df_expand = pd.DataFrame({})\n",
    "\n",
    "# Iterate over all attributes\n",
    "for column in data.columns:\n",
    "    # check if attribute is categorical, has less than 8 unique values, or is string values\n",
    "    if len(data[column].unique()) < 8 or data[column].dtype == np.dtype('object'):\n",
    "        encoding = pd.get_dummies(data[column])\n",
    "        x_df_expand = pd.concat([x_df_expand, encoding], axis=1)\n",
    "    else:\n",
    "        x_df_expand = pd.concat([x_df_expand, x_df_expand[[column]]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numpy = x_df_expanded.as_matrix(columns=None)\n",
    "twent_five = int(len(df_numpy)*(0.25))\n",
    "training, test = df_numpy[:twent_five,:], df_numpy[twent_five:,:]\n",
    "\n",
    "x_train = training[:,:-1]\n",
    "y_train = training[:,-1]\n",
    "x_test = test[:,:-1]\n",
    "y_test = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximize R^2 \n",
    "# use y test \n",
    "# 7 per envelope"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
